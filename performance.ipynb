{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install requests pandas\n",
        "!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBYaFVDC9W0Q",
        "outputId": "07cf748f-7255-4713-8d34-0f43fa165045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.164.0)\n",
            "Collecting google-api-python-client\n",
            "  Downloading google_api_python_client-2.166.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.38.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.24.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.69.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.1.31)\n",
            "Downloading google_api_python_client-2.166.0-py2.py3-none-any.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-api-python-client\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.164.0\n",
            "    Uninstalling google-api-python-client-2.164.0:\n",
            "      Successfully uninstalled google-api-python-client-2.164.0\n",
            "Successfully installed google-api-python-client-2.166.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create directory structure\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QJkSDU_A2chi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4SidIBcynFh",
        "outputId": "0fd8d755-6a8d-439b-aa8f-e96620d81d44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory structure created under 'ETL_Pipeline_Sabih_DS-59'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Base directory\n",
        "base_dir = \"ETL_Pipeline_Sabih_DS-59\"\n",
        "\n",
        "# Directory structure\n",
        "structure = {\n",
        "    \"\": [\"etl_pipeline.py\", \"scheduler.py\", \"requirements.txt\", \"README.md\", \"load_to_db.py\", \"report.pdf\"],\n",
        "    \"config\": [\"db_config.json\"],\n",
        "    \"data\": [\"sample_data.csv\", \"weather_data.json\", \"google_sheet_sample.csv\", 'weather_data.db'],\n",
        "    \"output\": [\"final_cleaned_data.csv\"],\n",
        "    \".github/workflows\": [\"ci_cd.yml\"]\n",
        "}\n",
        "\n",
        "# Create directories and files\n",
        "for folder, files in structure.items():\n",
        "    dir_path = os.path.join(base_dir, folder)\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "    for file in files:\n",
        "        file_path = os.path.join(dir_path, file)\n",
        "        with open(file_path, \"w\") as f:\n",
        "            pass  # Creates an empty file\n",
        "\n",
        "print(f\"Directory structure created under '{base_dir}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python_code = \"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# # Paths\n",
        "# BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "# DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
        "# OUTPUT_DIR = os.path.join(BASE_DIR, 'output')\n",
        "# CONFIG_DIR = os.path.join(BASE_DIR, 'config')\n",
        "\n",
        "\n",
        "def getSheetsData():\n",
        "  # Replace with your actual spreadsheet ID and sheet ID (GID)\n",
        "  spreadsheet_id = \"1DIXLTQfPB76206gklGInqhRzg5KW_uHbGXmwkBVg56k\"\n",
        "  sheet_id = \"1229579343\"  # Sheet ID (GID)\n",
        "\n",
        "  # Construct the export URL for CSV format\n",
        "  url = f\"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export?format=csv&gid={sheet_id}\"\n",
        "\n",
        "  # Send a GET request to the URL\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # Check if the request was successful (status code 200)\n",
        "  if response.status_code == 200:\n",
        "    # Save to a local file\n",
        "    print(response.content)\n",
        "    with open('/content/ETL_Pipeline_Sabih_DS-59/data/google_sheet_sample.csv', 'wb') as f:\n",
        "        f.write(response.content)\n",
        "  else:\n",
        "    print(f\"Failed to download the CSV file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "def save_Json_data(data, filename):\n",
        "    # Filter data to only include timestamp and temperature\n",
        "    filtered_data = [{'timestamp': entry['timestamp'], 'temperature': entry['temperature']} for entry in data]\n",
        "\n",
        "    # Save to JSON file\n",
        "    with open(filename, 'w') as json_file:\n",
        "        json.dump(filtered_data, json_file, indent=4)\n",
        "\n",
        "    print(f\"Data saved as {filename}.\")\n",
        "\n",
        "\n",
        "def fetch_forecast(city, api_key):\n",
        "    # Use the forecast endpoint (5 day / 3 hour forecast)\n",
        "    url = f'http://api.openweathermap.org/data/2.5/forecast?q={city}&appid={api_key}&units=metric'  # Use city name and `units=metric` for temperature in Celsius\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Error fetching weather data: {response.status_code}\")\n",
        "\n",
        "    data = response.json()\n",
        "\n",
        "    # Check if the response contains an error code\n",
        "    if data.get(\"cod\") != \"200\":\n",
        "        error_message = data.get(\"message\", \"Unknown error\")\n",
        "        raise Exception(f\"Error fetching weather data: {error_message}\")\n",
        "\n",
        "    # Extract forecast entries\n",
        "    forecast_entries = []\n",
        "    for entry in data.get('list', []):\n",
        "        try:\n",
        "            # Convert Unix timestamp to human-readable format\n",
        "            timestamp = datetime.utcfromtimestamp(entry['dt']).strftime('%Y-%m-%d %H:%M:%S')\n",
        "            temperature = entry['main']['temp']\n",
        "            forecast_entries.append({\n",
        "                'timestamp': timestamp,\n",
        "                'temperature': temperature,\n",
        "                'city': city\n",
        "            })\n",
        "        except KeyError as e:\n",
        "            print(f\"Key error {e} in entry: {entry}\")\n",
        "\n",
        "    return forecast_entries\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transform_data(csv_data, weather_data, sheet_data):\n",
        "    print(\"Starting transformation process\")\n",
        "\n",
        "    # Example: Clean CSV data\n",
        "    csv_data.dropna(inplace=True)\n",
        "\n",
        "    # Example: Convert weather JSON to DataFrame (assuming a list of weather records)\n",
        "    weather_df = pd.DataFrame(weather_data.get('weather', []))\n",
        "\n",
        "    # Example: Clean Google Sheet data\n",
        "    sheet_data.dropna(inplace=True)\n",
        "\n",
        "    # Example merge (if there is a common key, here assumed as 'id')\n",
        "    if 'id' in csv_data.columns and 'id' in sheet_data.columns:\n",
        "        merged_data = pd.merge(csv_data, sheet_data, on='id', how='inner')\n",
        "    else:\n",
        "        merged_data = csv_data.copy()\n",
        "\n",
        "    # Optionally, join weather info (here we simply add a new column with a summary)\n",
        "    if not weather_df.empty:\n",
        "        merged_data['weather_summary'] = weather_df.iloc[0].get('description', 'No data')\n",
        "    else:\n",
        "        merged_data['weather_summary'] = 'No data'\n",
        "\n",
        "    print(\"Transformation complete\")\n",
        "    return merged_data\n",
        "\n",
        "def load_data_to_file(dataframe, filename):\n",
        "    output_path = os.path.join(OUTPUT_DIR, filename)\n",
        "    print(f\"Loading cleaned data to {output_path}\")\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    dataframe.to_csv(output_path, index=False)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Extraction step\n",
        "\n",
        "\n",
        "    getSheetsData() # update sheet data\n",
        "    weather_data = fetch_forecast('london', 'cdc23585344f54d1d00caef6a3cffb60')\n",
        "\n",
        "\n",
        "    sheet_data = extract_csv('/content/ETL_Pipeline_Sabih_DS-59/data/google_sheet_sample.csv')\n",
        "    csv_data = extract_csv('/content/ETL_Pipeline_Sabih_DS-59/data/sample_data.csv')\n",
        "    weather_data = extract_json('sample_weather.json')\n",
        "    save_Json_data(weather_data, '/content/ETL_Pipeline_Sabih_DS-59/data/weather_data.json')\n",
        "\n",
        "    df_combined = pd.merge(df_csv1, df_csv2, on='common_column', how='outer')\n",
        "    df_combined = pd.merge(df_combined, df_json, on='common_column', how='outer')\n",
        "    df_combined = pd.merge(df_combined, df_sqlite, on='common_column', how='outer')\n",
        "\n",
        "    final_data = transform_data(csv_data, weather_data, sheet_data)\n",
        "\n",
        "    # Load step\n",
        "    load_data_to_file(final_data, 'final_cleaned_data.csv')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Open (or create) a Python file to write the code\n",
        "with open('/content/ETL_Pipeline_Sabih_DS-59/etl_pipeline.py', 'w') as f:\n",
        "    f.write(python_code)\n",
        "\n",
        "print(\"Python code has been written to 'etl.py'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQGGuuwa2Zoe",
        "outputId": "71ae0252-2b66-43c8-ace3-b0830e55b6ca"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python code has been written to 'etl.py'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Python code as a string\n",
        "python_code = \"\"\"\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "CONFIG_DIR = os.path.join(BASE_DIR, 'config')\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, 'output')\n",
        "\n",
        "def get_db_config():\n",
        "    config_path = os.path.join(CONFIG_DIR, 'db_config.json')\n",
        "    with open(config_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_csv_to_db(csv_filename, db_connection):\n",
        "    csv_path = os.path.join(OUTPUT_DIR, csv_filename)\n",
        "    print(f\"Loading data from {csv_path} into database\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df.to_sql('final_data', db_connection, if_exists='replace', index=False)\n",
        "    print(\"Data successfully loaded into database\")\n",
        "\n",
        "def main():\n",
        "    config = get_db_config()\n",
        "    # For demonstration, using SQLite. Replace with your actual DB connection.\n",
        "    db_path = os.path.join(BASE_DIR, 'dummy_database.db')\n",
        "    conn = sqlite3.connect(db_path)\n",
        "\n",
        "    try:\n",
        "        load_csv_to_db('final_cleaned_data.csv', conn)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Open (or create) a Python file to write the code\n",
        "with open('/content/ETL_Pipeline_Sabih_DS-59/load_to_db.py', 'w') as f:\n",
        "    f.write(python_code)\n",
        "\n",
        "print(\"Python code has been written to 'load_to_db.py'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1mKN3GB4_7E",
        "outputId": "7b89e4eb-b58d-464a-9a4c-19e43483df22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python code has been written to 'load_to_db.py'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Python code as a string\n",
        "python_code = \"\"\"\n",
        "import schedule\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "def run_etl_pipeline():\n",
        "    print(\"Starting ETL pipeline...\")\n",
        "    # Run the ETL pipeline script\n",
        "    subprocess.run(['python', 'etl_pipeline.py'], check=True)\n",
        "    print(\"ETL pipeline completed.\")\n",
        "\n",
        "def main():\n",
        "    # Schedule to run ETL pipeline every 10 minutes\n",
        "    schedule.every(10).minutes.do(run_etl_pipeline)\n",
        "\n",
        "    print(\"Scheduler started. Waiting for scheduled jobs...\")\n",
        "    while True:\n",
        "        schedule.run_pending()\n",
        "        time.sleep(1)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Open (or create) a Python file to write the code\n",
        "with open('/content/ETL_Pipeline_Sabih_DS-59/scheduler.py', 'w') as f:\n",
        "    f.write(python_code)\n",
        "\n",
        "print(\"Python code has been written to 'pipeline.py'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8pZUwak4Vmb",
        "outputId": "cffd66d0-6553-4a55-fb45-a8bd83cbaee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python code has been written to 'pipeline.py'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "    \"database\": \"dummy_db\",\n",
        "    \"user\": \"dummy_user\",\n",
        "    \"password\": \"dummy_pass\",\n",
        "    \"host\": \"localhost\",\n",
        "    \"port\": 5432,\n",
        "    \"api_key\": \"dummy_api_key\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "dT1wNWKk4Vj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Python code as a string\n",
        "DBConfig = \"\"\"{\n",
        "    \"database\": \"dummy_db\",\n",
        "    \"user\": \"dummy_user\",\n",
        "    \"password\": \"dummy_pass\",\n",
        "    \"host\": \"localhost\",\n",
        "    \"port\": 5432,\n",
        "    \"api_key\": \"dummy_api_key\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Open (or create) a Python file to write the code\n",
        "with open('/content/ETL_Pipeline_Sabih_DS-59/config/db_config.json', 'w') as f:\n",
        "    f.write(DBConfig)\n",
        "\n",
        "print(\"Python code has been written to 'DBConfig.Json'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COgNcjlx4VhT",
        "outputId": "e019974e-a19f-4501-d47c-bc66016d0e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python code has been written to 'DBConfig.Json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OR_cdI8MFa0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cNw01Yx7Faxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s5f0LEyuFavO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uk4J2wImFas3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A2qNYqOvFaqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3M-_fo39Fan9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1a7nrI89FalT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gBR3BjTb4Vd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b1b15e88fa797225412429c1c50c122a1\">api.openweathermap.org/data/2.5/forecast?id&appid={API key}"
      ],
      "metadata": {
        "id": "Af7lhFTz9j__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create data for csv and Mysql Database and JSon\n",
        "\n"
      ],
      "metadata": {
        "id": "HPCkVoSQE9w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "import os\n",
        "import csv\n",
        "import sqlite3\n",
        "import json\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "\n",
        "API_KEY = 'cdc23585344f54d1d00caef6a3cffb60'  # Replace with your real API key\n",
        "CITY = 'London'\n",
        "CSV_FILE = '/content/ETL_Pipeline_Sabih_DS-59/data/sample_data.csv'\n",
        "SQLITE_DB_FILE = '/content/ETL_Pipeline_Sabih_DS-59/data/weather_data.db'\n",
        "JSON_File = '/content/ETL_Pipeline_Sabih_DS-59/data/weather_data.json'\n",
        "\n",
        "\n",
        "# --- STEP 1: Fetch weather data from OpenWeatherMap API ---\n",
        "\n",
        "# def fetch_weather(city, api_key):\n",
        "\n",
        "#     url = f'http://api.openweathermap.org/data/2.5/forecast?id=524901&appid={API_KEY}'\n",
        "#     response = requests.get(url)\n",
        "#     data = response.json()\n",
        "#     print(data)\n",
        "\n",
        "#     timestamp = datetime.utcfromtimestamp(data['dt']).strftime('%Y-%m-%d %H:%M:%S')\n",
        "#     temperature = data['main']['temp']\n",
        "#     return {'timestamp': timestamp, 'temperature': temperature, 'city': city}\n",
        "\n",
        "\n",
        "def fetch_forecast(city, api_key):\n",
        "    # Use the forecast endpoint (5 day / 3 hour forecast)\n",
        "    url = f'http://api.openweathermap.org/data/2.5/forecast?q={city}&appid={api_key}&units=metric'  # Use city name and `units=metric` for temperature in Celsius\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Error fetching weather data: {response.status_code}\")\n",
        "\n",
        "    data = response.json()\n",
        "\n",
        "    # Check if the response contains an error code\n",
        "    if data.get(\"cod\") != \"200\":\n",
        "        error_message = data.get(\"message\", \"Unknown error\")\n",
        "        raise Exception(f\"Error fetching weather data: {error_message}\")\n",
        "\n",
        "    # Extract forecast entries\n",
        "    forecast_entries = []\n",
        "    for entry in data.get('list', []):\n",
        "        try:\n",
        "            # Convert Unix timestamp to human-readable format\n",
        "            timestamp = datetime.utcfromtimestamp(entry['dt']).strftime('%Y-%m-%d %H:%M:%S')\n",
        "            temperature = entry['main']['temp']\n",
        "            forecast_entries.append({\n",
        "                'timestamp': timestamp,\n",
        "                'temperature': temperature,\n",
        "                'city': city\n",
        "            })\n",
        "        except KeyError as e:\n",
        "            print(f\"Key error {e} in entry: {entry}\")\n",
        "\n",
        "    return forecast_entries\n",
        "\n",
        "\n",
        "# --- STEP 2: Save to CSV ---\n",
        "\n",
        "\n",
        "def create_weather_csv(data, filename='weather_data.csv'):\n",
        "    # Ensure the directory exists\n",
        "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "    # Open the file in write mode\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # Write the header\n",
        "        writer.writerow(['timestamp', 'temperature'])\n",
        "\n",
        "        # Write the data\n",
        "        for entry in data:\n",
        "            writer.writerow([entry['timestamp'], entry['temperature']])\n",
        "\n",
        "# --- STEP 3: Save to SQLite ---\n",
        "\n",
        "def create_and_insert_weather_data(data):\n",
        "    # Connect to SQLite database (it will create the file if it doesn't exist)\n",
        "    conn = sqlite3.connect('temperature_data.db')\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Create table with timestamp and temperature columns if it doesn't exist\n",
        "    cursor.execute('''\n",
        "    CREATE TABLE IF NOT EXISTS weather_data (\n",
        "        timestamp TEXT,\n",
        "        temperature REAL\n",
        "    )\n",
        "    ''')\n",
        "\n",
        "    # Insert data into the table\n",
        "    for record in data:\n",
        "        cursor.execute('''\n",
        "        INSERT INTO weather_data (timestamp, temperature)\n",
        "        VALUES (?, ?)\n",
        "        ''', (record['timestamp'], record['temperature']))\n",
        "\n",
        "    # Commit the changes and close the connection\n",
        "    conn.commit()\n",
        "\n",
        "    # Optional: Verify by fetching all rows from the table\n",
        "    cursor.execute('SELECT * FROM weather_data')\n",
        "    rows = cursor.fetchall()\n",
        "\n",
        "    # Print the data (optional step for verification)\n",
        "    for row in rows:\n",
        "        print(row)\n",
        "\n",
        "    # Close the connection\n",
        "    conn.close()\n",
        "\n",
        "\n",
        "\n",
        "def save_temperature_data(data, filename):\n",
        "    # Filter data to only include timestamp and temperature\n",
        "    filtered_data = [{'timestamp': entry['timestamp'], 'temperature': entry['temperature']} for entry in data]\n",
        "\n",
        "    # Save to JSON file\n",
        "    with open(filename, 'w') as json_file:\n",
        "        json.dump(filtered_data, json_file, indent=4)\n",
        "\n",
        "    print(f\"Data saved as {filename}.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- MAIN ---\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    weather_data = fetch_forecast(CITY, API_KEY)\n",
        "    print(\"Weather data:\", weather_data)\n",
        "\n",
        "    create_weather_csv(weather_data, CSV_FILE)\n",
        "    create_and_insert_weather_data(weather_data)\n",
        "    save_temperature_data(weather_data, JSON_File)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzoVtuvu9j8t",
        "outputId": "ed7da6dc-177b-404a-d299-b9db5311670d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weather data: [{'timestamp': '2025-04-05 12:00:00', 'temperature': 12.27, 'city': 'London'}, {'timestamp': '2025-04-05 15:00:00', 'temperature': 14.68, 'city': 'London'}, {'timestamp': '2025-04-05 18:00:00', 'temperature': 11.54, 'city': 'London'}, {'timestamp': '2025-04-05 21:00:00', 'temperature': 7.23, 'city': 'London'}, {'timestamp': '2025-04-06 00:00:00', 'temperature': 5.12, 'city': 'London'}, {'timestamp': '2025-04-06 03:00:00', 'temperature': 7.03, 'city': 'London'}, {'timestamp': '2025-04-06 06:00:00', 'temperature': 7.09, 'city': 'London'}, {'timestamp': '2025-04-06 09:00:00', 'temperature': 10.97, 'city': 'London'}, {'timestamp': '2025-04-06 12:00:00', 'temperature': 13.42, 'city': 'London'}, {'timestamp': '2025-04-06 15:00:00', 'temperature': 13.06, 'city': 'London'}, {'timestamp': '2025-04-06 18:00:00', 'temperature': 10.5, 'city': 'London'}, {'timestamp': '2025-04-06 21:00:00', 'temperature': 8.11, 'city': 'London'}, {'timestamp': '2025-04-07 00:00:00', 'temperature': 7.01, 'city': 'London'}, {'timestamp': '2025-04-07 03:00:00', 'temperature': 6.36, 'city': 'London'}, {'timestamp': '2025-04-07 06:00:00', 'temperature': 5.79, 'city': 'London'}, {'timestamp': '2025-04-07 09:00:00', 'temperature': 10.64, 'city': 'London'}, {'timestamp': '2025-04-07 12:00:00', 'temperature': 14.41, 'city': 'London'}, {'timestamp': '2025-04-07 15:00:00', 'temperature': 15.02, 'city': 'London'}, {'timestamp': '2025-04-07 18:00:00', 'temperature': 12.26, 'city': 'London'}, {'timestamp': '2025-04-07 21:00:00', 'temperature': 9.14, 'city': 'London'}, {'timestamp': '2025-04-08 00:00:00', 'temperature': 7.79, 'city': 'London'}, {'timestamp': '2025-04-08 03:00:00', 'temperature': 6.89, 'city': 'London'}, {'timestamp': '2025-04-08 06:00:00', 'temperature': 6.47, 'city': 'London'}, {'timestamp': '2025-04-08 09:00:00', 'temperature': 11.29, 'city': 'London'}, {'timestamp': '2025-04-08 12:00:00', 'temperature': 15.09, 'city': 'London'}, {'timestamp': '2025-04-08 15:00:00', 'temperature': 16.56, 'city': 'London'}, {'timestamp': '2025-04-08 18:00:00', 'temperature': 14.28, 'city': 'London'}, {'timestamp': '2025-04-08 21:00:00', 'temperature': 9.56, 'city': 'London'}, {'timestamp': '2025-04-09 00:00:00', 'temperature': 7.54, 'city': 'London'}, {'timestamp': '2025-04-09 03:00:00', 'temperature': 5.88, 'city': 'London'}, {'timestamp': '2025-04-09 06:00:00', 'temperature': 5.5, 'city': 'London'}, {'timestamp': '2025-04-09 09:00:00', 'temperature': 9.5, 'city': 'London'}, {'timestamp': '2025-04-09 12:00:00', 'temperature': 14.14, 'city': 'London'}, {'timestamp': '2025-04-09 15:00:00', 'temperature': 15.79, 'city': 'London'}, {'timestamp': '2025-04-09 18:00:00', 'temperature': 13.53, 'city': 'London'}, {'timestamp': '2025-04-09 21:00:00', 'temperature': 9.18, 'city': 'London'}, {'timestamp': '2025-04-10 00:00:00', 'temperature': 7.61, 'city': 'London'}, {'timestamp': '2025-04-10 03:00:00', 'temperature': 6.59, 'city': 'London'}, {'timestamp': '2025-04-10 06:00:00', 'temperature': 7.03, 'city': 'London'}, {'timestamp': '2025-04-10 09:00:00', 'temperature': 10.61, 'city': 'London'}]\n",
            "('2025-04-05 09:00:00', 9.74)\n",
            "('2025-04-05 12:00:00', 11.41)\n",
            "('2025-04-05 15:00:00', 14.22)\n",
            "('2025-04-05 18:00:00', 11.54)\n",
            "('2025-04-05 21:00:00', 7.23)\n",
            "('2025-04-06 00:00:00', 5.12)\n",
            "('2025-04-06 03:00:00', 7.03)\n",
            "('2025-04-06 06:00:00', 7.09)\n",
            "('2025-04-06 09:00:00', 10.97)\n",
            "('2025-04-06 12:00:00', 13.42)\n",
            "('2025-04-06 15:00:00', 13.06)\n",
            "('2025-04-06 18:00:00', 10.5)\n",
            "('2025-04-06 21:00:00', 8.11)\n",
            "('2025-04-07 00:00:00', 7.01)\n",
            "('2025-04-07 03:00:00', 6.36)\n",
            "('2025-04-07 06:00:00', 5.79)\n",
            "('2025-04-07 09:00:00', 10.64)\n",
            "('2025-04-07 12:00:00', 14.41)\n",
            "('2025-04-07 15:00:00', 15.02)\n",
            "('2025-04-07 18:00:00', 12.26)\n",
            "('2025-04-07 21:00:00', 9.14)\n",
            "('2025-04-08 00:00:00', 7.79)\n",
            "('2025-04-08 03:00:00', 6.89)\n",
            "('2025-04-08 06:00:00', 6.47)\n",
            "('2025-04-08 09:00:00', 11.29)\n",
            "('2025-04-08 12:00:00', 15.09)\n",
            "('2025-04-08 15:00:00', 16.56)\n",
            "('2025-04-08 18:00:00', 14.28)\n",
            "('2025-04-08 21:00:00', 9.56)\n",
            "('2025-04-09 00:00:00', 7.54)\n",
            "('2025-04-09 03:00:00', 5.88)\n",
            "('2025-04-09 06:00:00', 5.5)\n",
            "('2025-04-09 09:00:00', 9.5)\n",
            "('2025-04-09 12:00:00', 14.14)\n",
            "('2025-04-09 15:00:00', 15.79)\n",
            "('2025-04-09 18:00:00', 13.53)\n",
            "('2025-04-09 21:00:00', 9.18)\n",
            "('2025-04-10 00:00:00', 7.61)\n",
            "('2025-04-10 03:00:00', 6.59)\n",
            "('2025-04-10 06:00:00', 7.03)\n",
            "('2025-04-05 09:00:00', 9.95)\n",
            "('2025-04-05 12:00:00', 11.55)\n",
            "('2025-04-05 15:00:00', 14.29)\n",
            "('2025-04-05 18:00:00', 11.54)\n",
            "('2025-04-05 21:00:00', 7.23)\n",
            "('2025-04-06 00:00:00', 5.12)\n",
            "('2025-04-06 03:00:00', 7.03)\n",
            "('2025-04-06 06:00:00', 7.09)\n",
            "('2025-04-06 09:00:00', 10.97)\n",
            "('2025-04-06 12:00:00', 13.42)\n",
            "('2025-04-06 15:00:00', 13.06)\n",
            "('2025-04-06 18:00:00', 10.5)\n",
            "('2025-04-06 21:00:00', 8.11)\n",
            "('2025-04-07 00:00:00', 7.01)\n",
            "('2025-04-07 03:00:00', 6.36)\n",
            "('2025-04-07 06:00:00', 5.79)\n",
            "('2025-04-07 09:00:00', 10.64)\n",
            "('2025-04-07 12:00:00', 14.41)\n",
            "('2025-04-07 15:00:00', 15.02)\n",
            "('2025-04-07 18:00:00', 12.26)\n",
            "('2025-04-07 21:00:00', 9.14)\n",
            "('2025-04-08 00:00:00', 7.79)\n",
            "('2025-04-08 03:00:00', 6.89)\n",
            "('2025-04-08 06:00:00', 6.47)\n",
            "('2025-04-08 09:00:00', 11.29)\n",
            "('2025-04-08 12:00:00', 15.09)\n",
            "('2025-04-08 15:00:00', 16.56)\n",
            "('2025-04-08 18:00:00', 14.28)\n",
            "('2025-04-08 21:00:00', 9.56)\n",
            "('2025-04-09 00:00:00', 7.54)\n",
            "('2025-04-09 03:00:00', 5.88)\n",
            "('2025-04-09 06:00:00', 5.5)\n",
            "('2025-04-09 09:00:00', 9.5)\n",
            "('2025-04-09 12:00:00', 14.14)\n",
            "('2025-04-09 15:00:00', 15.79)\n",
            "('2025-04-09 18:00:00', 13.53)\n",
            "('2025-04-09 21:00:00', 9.18)\n",
            "('2025-04-10 00:00:00', 7.61)\n",
            "('2025-04-10 03:00:00', 6.59)\n",
            "('2025-04-10 06:00:00', 7.03)\n",
            "('2025-04-05 12:00:00', 11.86)\n",
            "('2025-04-05 15:00:00', 14.43)\n",
            "('2025-04-05 18:00:00', 11.54)\n",
            "('2025-04-05 21:00:00', 7.23)\n",
            "('2025-04-06 00:00:00', 5.12)\n",
            "('2025-04-06 03:00:00', 7.03)\n",
            "('2025-04-06 06:00:00', 7.09)\n",
            "('2025-04-06 09:00:00', 10.97)\n",
            "('2025-04-06 12:00:00', 13.42)\n",
            "('2025-04-06 15:00:00', 13.06)\n",
            "('2025-04-06 18:00:00', 10.5)\n",
            "('2025-04-06 21:00:00', 8.11)\n",
            "('2025-04-07 00:00:00', 7.01)\n",
            "('2025-04-07 03:00:00', 6.36)\n",
            "('2025-04-07 06:00:00', 5.79)\n",
            "('2025-04-07 09:00:00', 10.64)\n",
            "('2025-04-07 12:00:00', 14.41)\n",
            "('2025-04-07 15:00:00', 15.02)\n",
            "('2025-04-07 18:00:00', 12.26)\n",
            "('2025-04-07 21:00:00', 9.14)\n",
            "('2025-04-08 00:00:00', 7.79)\n",
            "('2025-04-08 03:00:00', 6.89)\n",
            "('2025-04-08 06:00:00', 6.47)\n",
            "('2025-04-08 09:00:00', 11.29)\n",
            "('2025-04-08 12:00:00', 15.09)\n",
            "('2025-04-08 15:00:00', 16.56)\n",
            "('2025-04-08 18:00:00', 14.28)\n",
            "('2025-04-08 21:00:00', 9.56)\n",
            "('2025-04-09 00:00:00', 7.54)\n",
            "('2025-04-09 03:00:00', 5.88)\n",
            "('2025-04-09 06:00:00', 5.5)\n",
            "('2025-04-09 09:00:00', 9.5)\n",
            "('2025-04-09 12:00:00', 14.14)\n",
            "('2025-04-09 15:00:00', 15.79)\n",
            "('2025-04-09 18:00:00', 13.53)\n",
            "('2025-04-09 21:00:00', 9.18)\n",
            "('2025-04-10 00:00:00', 7.61)\n",
            "('2025-04-10 03:00:00', 6.59)\n",
            "('2025-04-10 06:00:00', 7.03)\n",
            "('2025-04-10 09:00:00', 10.61)\n",
            "('2025-04-05 12:00:00', 11.97)\n",
            "('2025-04-05 15:00:00', 14.55)\n",
            "('2025-04-05 18:00:00', 11.54)\n",
            "('2025-04-05 21:00:00', 7.23)\n",
            "('2025-04-06 00:00:00', 5.12)\n",
            "('2025-04-06 03:00:00', 7.03)\n",
            "('2025-04-06 06:00:00', 7.09)\n",
            "('2025-04-06 09:00:00', 10.97)\n",
            "('2025-04-06 12:00:00', 13.42)\n",
            "('2025-04-06 15:00:00', 13.06)\n",
            "('2025-04-06 18:00:00', 10.5)\n",
            "('2025-04-06 21:00:00', 8.11)\n",
            "('2025-04-07 00:00:00', 7.01)\n",
            "('2025-04-07 03:00:00', 6.36)\n",
            "('2025-04-07 06:00:00', 5.79)\n",
            "('2025-04-07 09:00:00', 10.64)\n",
            "('2025-04-07 12:00:00', 14.41)\n",
            "('2025-04-07 15:00:00', 15.02)\n",
            "('2025-04-07 18:00:00', 12.26)\n",
            "('2025-04-07 21:00:00', 9.14)\n",
            "('2025-04-08 00:00:00', 7.79)\n",
            "('2025-04-08 03:00:00', 6.89)\n",
            "('2025-04-08 06:00:00', 6.47)\n",
            "('2025-04-08 09:00:00', 11.29)\n",
            "('2025-04-08 12:00:00', 15.09)\n",
            "('2025-04-08 15:00:00', 16.56)\n",
            "('2025-04-08 18:00:00', 14.28)\n",
            "('2025-04-08 21:00:00', 9.56)\n",
            "('2025-04-09 00:00:00', 7.54)\n",
            "('2025-04-09 03:00:00', 5.88)\n",
            "('2025-04-09 06:00:00', 5.5)\n",
            "('2025-04-09 09:00:00', 9.5)\n",
            "('2025-04-09 12:00:00', 14.14)\n",
            "('2025-04-09 15:00:00', 15.79)\n",
            "('2025-04-09 18:00:00', 13.53)\n",
            "('2025-04-09 21:00:00', 9.18)\n",
            "('2025-04-10 00:00:00', 7.61)\n",
            "('2025-04-10 03:00:00', 6.59)\n",
            "('2025-04-10 06:00:00', 7.03)\n",
            "('2025-04-10 09:00:00', 10.61)\n",
            "('2025-04-05 12:00:00', 11.97)\n",
            "('2025-04-05 15:00:00', 14.55)\n",
            "('2025-04-05 18:00:00', 11.54)\n",
            "('2025-04-05 21:00:00', 7.23)\n",
            "('2025-04-06 00:00:00', 5.12)\n",
            "('2025-04-06 03:00:00', 7.03)\n",
            "('2025-04-06 06:00:00', 7.09)\n",
            "('2025-04-06 09:00:00', 10.97)\n",
            "('2025-04-06 12:00:00', 13.42)\n",
            "('2025-04-06 15:00:00', 13.06)\n",
            "('2025-04-06 18:00:00', 10.5)\n",
            "('2025-04-06 21:00:00', 8.11)\n",
            "('2025-04-07 00:00:00', 7.01)\n",
            "('2025-04-07 03:00:00', 6.36)\n",
            "('2025-04-07 06:00:00', 5.79)\n",
            "('2025-04-07 09:00:00', 10.64)\n",
            "('2025-04-07 12:00:00', 14.41)\n",
            "('2025-04-07 15:00:00', 15.02)\n",
            "('2025-04-07 18:00:00', 12.26)\n",
            "('2025-04-07 21:00:00', 9.14)\n",
            "('2025-04-08 00:00:00', 7.79)\n",
            "('2025-04-08 03:00:00', 6.89)\n",
            "('2025-04-08 06:00:00', 6.47)\n",
            "('2025-04-08 09:00:00', 11.29)\n",
            "('2025-04-08 12:00:00', 15.09)\n",
            "('2025-04-08 15:00:00', 16.56)\n",
            "('2025-04-08 18:00:00', 14.28)\n",
            "('2025-04-08 21:00:00', 9.56)\n",
            "('2025-04-09 00:00:00', 7.54)\n",
            "('2025-04-09 03:00:00', 5.88)\n",
            "('2025-04-09 06:00:00', 5.5)\n",
            "('2025-04-09 09:00:00', 9.5)\n",
            "('2025-04-09 12:00:00', 14.14)\n",
            "('2025-04-09 15:00:00', 15.79)\n",
            "('2025-04-09 18:00:00', 13.53)\n",
            "('2025-04-09 21:00:00', 9.18)\n",
            "('2025-04-10 00:00:00', 7.61)\n",
            "('2025-04-10 03:00:00', 6.59)\n",
            "('2025-04-10 06:00:00', 7.03)\n",
            "('2025-04-10 09:00:00', 10.61)\n",
            "('2025-04-05 12:00:00', 12.27)\n",
            "('2025-04-05 15:00:00', 14.68)\n",
            "('2025-04-05 18:00:00', 11.54)\n",
            "('2025-04-05 21:00:00', 7.23)\n",
            "('2025-04-06 00:00:00', 5.12)\n",
            "('2025-04-06 03:00:00', 7.03)\n",
            "('2025-04-06 06:00:00', 7.09)\n",
            "('2025-04-06 09:00:00', 10.97)\n",
            "('2025-04-06 12:00:00', 13.42)\n",
            "('2025-04-06 15:00:00', 13.06)\n",
            "('2025-04-06 18:00:00', 10.5)\n",
            "('2025-04-06 21:00:00', 8.11)\n",
            "('2025-04-07 00:00:00', 7.01)\n",
            "('2025-04-07 03:00:00', 6.36)\n",
            "('2025-04-07 06:00:00', 5.79)\n",
            "('2025-04-07 09:00:00', 10.64)\n",
            "('2025-04-07 12:00:00', 14.41)\n",
            "('2025-04-07 15:00:00', 15.02)\n",
            "('2025-04-07 18:00:00', 12.26)\n",
            "('2025-04-07 21:00:00', 9.14)\n",
            "('2025-04-08 00:00:00', 7.79)\n",
            "('2025-04-08 03:00:00', 6.89)\n",
            "('2025-04-08 06:00:00', 6.47)\n",
            "('2025-04-08 09:00:00', 11.29)\n",
            "('2025-04-08 12:00:00', 15.09)\n",
            "('2025-04-08 15:00:00', 16.56)\n",
            "('2025-04-08 18:00:00', 14.28)\n",
            "('2025-04-08 21:00:00', 9.56)\n",
            "('2025-04-09 00:00:00', 7.54)\n",
            "('2025-04-09 03:00:00', 5.88)\n",
            "('2025-04-09 06:00:00', 5.5)\n",
            "('2025-04-09 09:00:00', 9.5)\n",
            "('2025-04-09 12:00:00', 14.14)\n",
            "('2025-04-09 15:00:00', 15.79)\n",
            "('2025-04-09 18:00:00', 13.53)\n",
            "('2025-04-09 21:00:00', 9.18)\n",
            "('2025-04-10 00:00:00', 7.61)\n",
            "('2025-04-10 03:00:00', 6.59)\n",
            "('2025-04-10 06:00:00', 7.03)\n",
            "('2025-04-10 09:00:00', 10.61)\n",
            "('2025-04-05 12:00:00', 12.27)\n",
            "('2025-04-05 15:00:00', 14.68)\n",
            "('2025-04-05 18:00:00', 11.54)\n",
            "('2025-04-05 21:00:00', 7.23)\n",
            "('2025-04-06 00:00:00', 5.12)\n",
            "('2025-04-06 03:00:00', 7.03)\n",
            "('2025-04-06 06:00:00', 7.09)\n",
            "('2025-04-06 09:00:00', 10.97)\n",
            "('2025-04-06 12:00:00', 13.42)\n",
            "('2025-04-06 15:00:00', 13.06)\n",
            "('2025-04-06 18:00:00', 10.5)\n",
            "('2025-04-06 21:00:00', 8.11)\n",
            "('2025-04-07 00:00:00', 7.01)\n",
            "('2025-04-07 03:00:00', 6.36)\n",
            "('2025-04-07 06:00:00', 5.79)\n",
            "('2025-04-07 09:00:00', 10.64)\n",
            "('2025-04-07 12:00:00', 14.41)\n",
            "('2025-04-07 15:00:00', 15.02)\n",
            "('2025-04-07 18:00:00', 12.26)\n",
            "('2025-04-07 21:00:00', 9.14)\n",
            "('2025-04-08 00:00:00', 7.79)\n",
            "('2025-04-08 03:00:00', 6.89)\n",
            "('2025-04-08 06:00:00', 6.47)\n",
            "('2025-04-08 09:00:00', 11.29)\n",
            "('2025-04-08 12:00:00', 15.09)\n",
            "('2025-04-08 15:00:00', 16.56)\n",
            "('2025-04-08 18:00:00', 14.28)\n",
            "('2025-04-08 21:00:00', 9.56)\n",
            "('2025-04-09 00:00:00', 7.54)\n",
            "('2025-04-09 03:00:00', 5.88)\n",
            "('2025-04-09 06:00:00', 5.5)\n",
            "('2025-04-09 09:00:00', 9.5)\n",
            "('2025-04-09 12:00:00', 14.14)\n",
            "('2025-04-09 15:00:00', 15.79)\n",
            "('2025-04-09 18:00:00', 13.53)\n",
            "('2025-04-09 21:00:00', 9.18)\n",
            "('2025-04-10 00:00:00', 7.61)\n",
            "('2025-04-10 03:00:00', 6.59)\n",
            "('2025-04-10 06:00:00', 7.03)\n",
            "('2025-04-10 09:00:00', 10.61)\n",
            "('2025-04-05 12:00:00', 12.27)\n",
            "('2025-04-05 15:00:00', 14.68)\n",
            "('2025-04-05 18:00:00', 11.54)\n",
            "('2025-04-05 21:00:00', 7.23)\n",
            "('2025-04-06 00:00:00', 5.12)\n",
            "('2025-04-06 03:00:00', 7.03)\n",
            "('2025-04-06 06:00:00', 7.09)\n",
            "('2025-04-06 09:00:00', 10.97)\n",
            "('2025-04-06 12:00:00', 13.42)\n",
            "('2025-04-06 15:00:00', 13.06)\n",
            "('2025-04-06 18:00:00', 10.5)\n",
            "('2025-04-06 21:00:00', 8.11)\n",
            "('2025-04-07 00:00:00', 7.01)\n",
            "('2025-04-07 03:00:00', 6.36)\n",
            "('2025-04-07 06:00:00', 5.79)\n",
            "('2025-04-07 09:00:00', 10.64)\n",
            "('2025-04-07 12:00:00', 14.41)\n",
            "('2025-04-07 15:00:00', 15.02)\n",
            "('2025-04-07 18:00:00', 12.26)\n",
            "('2025-04-07 21:00:00', 9.14)\n",
            "('2025-04-08 00:00:00', 7.79)\n",
            "('2025-04-08 03:00:00', 6.89)\n",
            "('2025-04-08 06:00:00', 6.47)\n",
            "('2025-04-08 09:00:00', 11.29)\n",
            "('2025-04-08 12:00:00', 15.09)\n",
            "('2025-04-08 15:00:00', 16.56)\n",
            "('2025-04-08 18:00:00', 14.28)\n",
            "('2025-04-08 21:00:00', 9.56)\n",
            "('2025-04-09 00:00:00', 7.54)\n",
            "('2025-04-09 03:00:00', 5.88)\n",
            "('2025-04-09 06:00:00', 5.5)\n",
            "('2025-04-09 09:00:00', 9.5)\n",
            "('2025-04-09 12:00:00', 14.14)\n",
            "('2025-04-09 15:00:00', 15.79)\n",
            "('2025-04-09 18:00:00', 13.53)\n",
            "('2025-04-09 21:00:00', 9.18)\n",
            "('2025-04-10 00:00:00', 7.61)\n",
            "('2025-04-10 03:00:00', 6.59)\n",
            "('2025-04-10 06:00:00', 7.03)\n",
            "('2025-04-10 09:00:00', 10.61)\n",
            "Data saved as /content/ETL_Pipeline_Sabih_DS-59/data/weather_data.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get data from google sheets"
      ],
      "metadata": {
        "id": "55G5kdGlHqx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Replace with your actual spreadsheet ID and sheet ID (GID)\n",
        "spreadsheet_id = \"1DIXLTQfPB76206gklGInqhRzg5KW_uHbGXmwkBVg56k\"\n",
        "sheet_id = \"1229579343\"  # Sheet ID (GID)\n",
        "\n",
        "# Construct the export URL for CSV format\n",
        "url = f\"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export?format=csv&gid={sheet_id}\"\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Save to a local file\n",
        "    print(response.content)\n",
        "    with open('/content/ETL_Pipeline_Sabih_DS-59/data/google_sheet_sample.csv', 'wb') as f:\n",
        "        f.write(response.content)\n",
        "else:\n",
        "    print(f\"Failed to download the CSV file. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3Dhy_OZ9j6Z",
        "outputId": "4e623e37-a04b-4a5e-cdb3-38fb63d7077a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'timestamp,temperature\\r\\n2025-04-05 9:00:00,8.83\\r\\n2025-04-05 12:00:00,10.78\\r\\n2025-04-05 15:00:00,13.92\\r\\n2025-04-05 18:00:00,11.54\\r\\n2025-04-05 21:00:00,7.23\\r\\n2025-04-06 0:00:00,5.12\\r\\n2025-04-06 3:00:00,7.03\\r\\n2025-04-06 6:00:00,7.09\\r\\n2025-04-06 9:00:00,10.97\\r\\n2025-04-06 12:00:00,13.42\\r\\n2025-04-06 15:00:00,13.06\\r\\n2025-04-06 18:00:00,10.5\\r\\n2025-04-06 21:00:00,8.11\\r\\n2025-04-07 0:00:00,7.01\\r\\n2025-04-07 3:00:00,6.36\\r\\n2025-04-07 6:00:00,5.79\\r\\n2025-04-07 9:00:00,10.64\\r\\n2025-04-07 12:00:00,14.41\\r\\n2025-04-07 15:00:00,15.02\\r\\n2025-04-07 18:00:00,12.26\\r\\n2025-04-07 21:00:00,9.14\\r\\n2025-04-08 0:00:00,7.79\\r\\n2025-04-08 3:00:00,6.89\\r\\n2025-04-08 6:00:00,6.47\\r\\n2025-04-08 9:00:00,11.29\\r\\n2025-04-08 12:00:00,15.09\\r\\n2025-04-08 15:00:00,16.56\\r\\n2025-04-08 18:00:00,14.28\\r\\n2025-04-08 21:00:00,9.56\\r\\n2025-04-09 0:00:00,7.54\\r\\n2025-04-09 3:00:00,5.88\\r\\n2025-04-09 6:00:00,5.5\\r\\n2025-04-09 9:00:00,9.5\\r\\n2025-04-09 12:00:00,14.14\\r\\n2025-04-09 15:00:00,15.79\\r\\n2025-04-09 18:00:00,13.53\\r\\n2025-04-09 21:00:00,9.18\\r\\n2025-04-10 0:00:00,7.61\\r\\n2025-04-10 3:00:00,6.59\\r\\n2025-04-10 6:00:00,7.03'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "whgmLua-9j4M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}